# Understanding Resilient Distributed Datasets

RDDs are collections of records which are kept in memory and they are the primary way which we interact with data in Spark. Three special characteristics of RDDs are: *They have partitions*, *they are read-only*, and *they have lineage*.

## Partitions

An RDD is nothing but a collection of records that represents data which is kept in memory. There is a limit to how much data you can keep in memory on a single machine, so if you want to deal with a large amount of data then you will need multiple machines. This data in one RDD is divided into partitions that are kept in multiple machines. Each partition is stored on a different node. Nodes are the individual machines in a cluster. The advantage of this is that you can have your entire dataset in memory as long as the data size does not exceed the combined memory size of the cluster. Since the data is kept on multiple machines, it can now be processed in parallel which is very efficient. Specific types of operations that can be performed in parallel are the ones that should be performed on RDDs. Any operations that requires you to keep track of which record in the dataset you are using and loop through the individual records in the dataset are not optimized.

## Read-Only

RDDs are immutable. If you want to do something to an RDD then there are two types of operations you can use. One is a **Transformation** which converts an RDD into another RDD. The other is an **Action** which requests a result or reads data from the RDD. You cannot modify an RDD in place.

**Transformations** are used to transform records in a dataset. Whether it is filtering out only certain rows, or extracting specific fields, or doing something with those fields. Once a dataset is loaded into an RDD you might want to perform a chain of operations before you actually look at some values. You can define this complete set of transformations upfront. For example, you might want to: Load data, Pick only the 3rd column, then sort the values and return only the top 10 rows. Each of these will require a separate RDD operation and each operations will convert one RDD to another RDD. However, the actual data processing doesn't happen immediately. It happens only when the user actually requests the results - when you request for the tep 10 rows. As you are defining each of these steps, intermediate RDDs are created but those RDDs only contain metadata at this point. The data processing will only happen when an action is requested. An action action could involve reading a specific set of rows, computing a count, or computing a sum. Either way the action means that you want to see some value as a result of the data processing task. When this action is requested, the chain of transformations that you defined earlier loads the data. In the meantime before the action, Spark is just keeping record of the series of transformations requested by the user. When the action is called upon, Spark will group the transformations in an efficient way and perform the data processing utilizing the resources it has in the best manner. 

## Lineage

Lineage is the way that Spark groups all of the transformations before an action is called and and optimizes them once an action is called (Lazy Evaluation). When an RDD is created it just holds metadata which is: The transformation that created the RDD and the parent RDD from which the RDD was created. Every RDD knows where it came from. When the RDD is created it does not hold any real data - it contains only metadata at this point. Since we can trace the parents of each RDD, we can go all the way back from a child RDD to a parent RDD to the CSV used to create the parent RDD from using just the child RDD's metadata. Once an action is requested all of the parent RDDs are materialized starting with the original CSV, then the parent RDD, then the child RDD. **Lineage** is what allows RDDs to have resilience and lazy evaluation. Resilience means that even when there is a fault, of if one of the nodes in the cluster crashes, you can reconstruct the RDD from the source data because you know the complete set of transformations involved in creating that RDD. Lazy evaluation allows you to perform an operation only when it is necessary, that is when the user is looking for some result. Otherwise, you just keep the series of transformations in memory as metadata and you do not have to occupy the resources in memory until then.
