# Transforming & Cleaning Unstructured Data

To transform data in Spark you use the **Functional Paradigm**. An RDD is a collection of records. In other languages you might use an **Imperative Paradigm** to iterate through our data which involves using loops. You would say: for each element, perform some transformation on it, then move on to the next element and repeat the process until you reach the end of the collection. The Imperative way allows you to perform an operation sequentially on each element of the collection. This allows you to keep track of which element you are on, how many elements you have already finished, and how many are left to complete but it doesn't involve any parallelism and might not be taking advantage of the performance optimization of distributed computing. 